Initially, I was using only one convolutional-pooling pair. This yielded me an accuracy of 5%. When I started using another convolutonal-pooling pair, it brought the accuracy up to 96%. The order of the convolutional and pooling layers does not seem to matter much. Further increases to the number of convolutional-pooling pairs yielded diminishing returns. When I made the filters bigger ((3,3) --> (3,5)) while maintaining the ratio, it performed significantly less well. Changing the filter side length ratio while maintaining the area reduced the accuracy to 67% from 96%. Increasing the size of the pool in the pooling layers exponentially decreased the accuracy. Increasing the number of hidden layers seemed to have minimal effect. Changing the size of the hidden layer from 128 to anything else was detrimental. Reducing the dropout percentage made it rely heavily on certain nodes which made it do really well in some epochs but not on the final test.